This is my attempt at a GPU accelerated machine learning engine

The example I'm using for learning purposes is a custom genetic algorithm that I've just botched together. It's still a work in progress, but I hope by the end of it to have a fully GPU-accelerated server instance that I can run on a computer, and feed it training data from a seperate outside PC in realtime. It's designed to be as modular as possible, utilising the CPU to save a copy of the model to disk (And a backup copy) while the GPU trains up the next instance of the model. The GPU and CPU are constantly cycling data. So as soon as the CPU is finished saving the model to disk, the GPU hands the CPU/RAM a copy of the new updated version of the model that's been trained further before continuing on with training. Meanwhile the CPU/RAM now have an updated version of the model and starts saving it and it's backup to disk. This is designed from the ground up to absolutely ABUSE all compute processing power at my codes disposal. Maybe later I might change it out from using OpenGL compute shaders to something more like OpenCL if I can work it out

The current issue I'm working on, is allowing significantly more processing power by converting from 32-bit to 64-bit. If I can get my hands on an RTX4090 then this change will make my code run 12 times faster than it does now in 32-bit world